{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpYq9D76523j",
        "outputId": "fdc4092c-5e92-48c1-f4d6-e69de31d042e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install sklearn\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioT4XA14YqPn",
        "outputId": "0e3b6a6b-5ef3-4078-b1db-b6138a7dccfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import regex as re\n",
        "from sklearn.metrics import *\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "import string\n",
        "import operator\n",
        "import os\n",
        "import requests\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords') # If needed\n",
        "nltk.download('punkt') # If needed\n",
        "nltk.download('wordnet') # If needed\n",
        "# nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80XPOHGfG4YL"
      },
      "source": [
        "# Importing data from drive\n",
        "\n",
        "This cell tries to connect to google drive if you click on cancel it will pick the csv file from git repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbSya2mmIKLp",
        "outputId": "92bc66a9-0a35-4a51-ae0b-f69338a62d6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive is not mounted so picking data from github url\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "sport            511\n",
              "business         510\n",
              "politics         417\n",
              "tech             401\n",
              "entertainment    386\n",
              "Name: category, dtype: int64"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "try: \n",
        "\n",
        "    from google.colab import drive\n",
        "\n",
        "    # Mount colab to google drive need to unconnect if we want to connect from google drive\n",
        "    # drive.mount(\"/content/gdrive\")\n",
        "\n",
        "\n",
        "    #List of categories\n",
        "    categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
        "\n",
        "    #Path in google drive\n",
        "    path = \"/content/gdrive/MyDrive/Colab Notebooks/text_cl/bbc/\"\n",
        "\n",
        "    total_data = []\n",
        "\n",
        "    #This loop iterates every folder and stores text content in total_data variable\n",
        "    for category in categories:\n",
        "        file_paths = os.listdir(path+category)\n",
        "        for name in file_paths:\n",
        "            file_name = \"{0}{1}/{2}\".format(path,category,name)\n",
        "            with open(file_name,'rb') as file:\n",
        "                text = file.read().decode(errors='replace').replace(\"\\n\",\" \")\n",
        "            total_data.append((category,name,text))\n",
        "\n",
        "  \n",
        "    # Creating the dataframe which contains all the text content\n",
        "    df = pd.DataFrame(total_data)\n",
        "\n",
        "\n",
        "except :\n",
        "  print(\"Drive is not mounted so picking data from github url\")\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/sunnybandlamudi/AML/master/bbc.csv\")  \n",
        "\n",
        "# Adding names to data columns\n",
        "df.columns=[\"category\",\"file_name\",\"text\"]\n",
        "\n",
        "\n",
        "# Displaying the total records present in each category\n",
        "df['category'].value_counts()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnSdP2_1HPOr"
      },
      "source": [
        "# Importing data from Git repository\n",
        "\n",
        "Stored all text content to csv file and uploaded to git repository.\n",
        "\n",
        "Github Link :https://raw.githubusercontent.com/sunnybandlamudi/AML/master/bbc.csv\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6qXfF28wvYGG"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/sunnybandlamudi/AML/master/bbc.csv\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6joS7eKNHmn0"
      },
      "source": [
        "# Displaying the data frame object\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "hdoEit04wZ3V",
        "outputId": "40b12dfa-025e-48bb-8763-b3c241e52400"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-34b5d0cf-c559-4514-9f9d-dc6e79d5722b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>file_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>business</td>\n",
              "      <td>289.txt</td>\n",
              "      <td>UK economy facing 'major risks'  The UK manufa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>504.txt</td>\n",
              "      <td>Aids and climate top Davos agenda  Climate cha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>business</td>\n",
              "      <td>262.txt</td>\n",
              "      <td>Asian quake hits European shares  Shares in Eu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>business</td>\n",
              "      <td>276.txt</td>\n",
              "      <td>India power shares jump on debut  Shares in In...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>business</td>\n",
              "      <td>510.txt</td>\n",
              "      <td>Lacroix label bought by US firm  Luxury goods ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>tech</td>\n",
              "      <td>086.txt</td>\n",
              "      <td>Warning over Windows Word files  Writing a Mic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>tech</td>\n",
              "      <td>253.txt</td>\n",
              "      <td>Fast lifts rise into record books  Two high-sp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2222</th>\n",
              "      <td>tech</td>\n",
              "      <td>247.txt</td>\n",
              "      <td>Nintendo adds media playing to DS  Nintendo is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2223</th>\n",
              "      <td>tech</td>\n",
              "      <td>290.txt</td>\n",
              "      <td>Fast moving phone viruses appear  Security fir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2224</th>\n",
              "      <td>tech</td>\n",
              "      <td>284.txt</td>\n",
              "      <td>Hacker threat to Apple's iTunes  Users of Appl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2225 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34b5d0cf-c559-4514-9f9d-dc6e79d5722b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-34b5d0cf-c559-4514-9f9d-dc6e79d5722b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-34b5d0cf-c559-4514-9f9d-dc6e79d5722b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      category file_name                                               text\n",
              "0     business   289.txt  UK economy facing 'major risks'  The UK manufa...\n",
              "1     business   504.txt  Aids and climate top Davos agenda  Climate cha...\n",
              "2     business   262.txt  Asian quake hits European shares  Shares in Eu...\n",
              "3     business   276.txt  India power shares jump on debut  Shares in In...\n",
              "4     business   510.txt  Lacroix label bought by US firm  Luxury goods ...\n",
              "...        ...       ...                                                ...\n",
              "2220      tech   086.txt  Warning over Windows Word files  Writing a Mic...\n",
              "2221      tech   253.txt  Fast lifts rise into record books  Two high-sp...\n",
              "2222      tech   247.txt  Nintendo adds media playing to DS  Nintendo is...\n",
              "2223      tech   290.txt  Fast moving phone viruses appear  Security fir...\n",
              "2224      tech   284.txt  Hacker threat to Apple's iTunes  Users of Appl...\n",
              "\n",
              "[2225 rows x 3 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Using to display data frame\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GADJbdRzHr6G"
      },
      "source": [
        "# Text prepossing\n",
        "\n",
        "`text_process` fucntion takes the text content as the input and returns the processed text.\n",
        "\n",
        "Creating a new `text_clean` column to stote the processed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COP2TirnIupm",
        "outputId": "5ec78653-b6df-4566-82aa-3f534c8e257b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning done\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#lemmatizer object\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "#set of english stop words\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "#Adding frequent characters to stopwords list\n",
        "stopwords.add(\".\")\n",
        "stopwords.add(\",\")\n",
        "stopwords.add(\"--\")\n",
        "stopwords.add(\"``\")\n",
        "\n",
        "# Function to returns the text of tokens from a string_data\n",
        "def text_process(string_data): \n",
        "\n",
        "    # separating every sentence present in string_data\n",
        "    sentence_split=nltk.tokenize.sent_tokenize(string_data)\n",
        "    \n",
        "    list_tokens=[]\n",
        "\n",
        "    #Iterating through every sentence\n",
        "    for sentence in sentence_split:\n",
        "\n",
        "        # Below line removes the puntuation \n",
        "        #ex:- \"The Government\" converts into The Goverment(without quotation marks)\n",
        "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        # Using word_tokenize function to tokenize the sentence. list_tokens_sentence contains all the token present in sentence variable\n",
        "        list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
        "\n",
        "        # Iterating through all the tokens\n",
        "        for token in list_tokens_sentence:\n",
        "            # If token present in stopwords set we are not adding it ot list_tokens\n",
        "            if token in stopwords:\n",
        "              continue;\n",
        "            \n",
        "            # we are lemmatizeing token, converting to lower case and adding it to list_tokens \n",
        "            list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
        "\n",
        "    # return concatenated string of all the tokens present in string_data\n",
        "    return \" \".join(list_tokens)\n",
        "\n",
        "\n",
        "# creating the new column text_clean which contains the processed string(Tokenization, Lemmatization, Stopwords removal)\n",
        "df['text_clean'] = df['text'].apply(lambda x: text_process(x))\n",
        "\n",
        "print(\"Cleaning done\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8oGUi3JIL0j"
      },
      "source": [
        "# Displaying dataframe after text processing\n",
        "\n",
        "`text_clean` column contains the processed text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "gPhoAO6kJKkm",
        "outputId": "f0431e6f-8cbf-46e7-82b6-528b92a8fe1b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b31022d8-d7e5-4aad-979a-1bc2c4c34f8c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>file_name</th>\n",
              "      <th>text</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>business</td>\n",
              "      <td>289.txt</td>\n",
              "      <td>UK economy facing 'major risks'  The UK manufa...</td>\n",
              "      <td>uk economy facing major risk the uk manufactur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>504.txt</td>\n",
              "      <td>Aids and climate top Davos agenda  Climate cha...</td>\n",
              "      <td>aids climate top davos agenda climate change f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>business</td>\n",
              "      <td>262.txt</td>\n",
              "      <td>Asian quake hits European shares  Shares in Eu...</td>\n",
              "      <td>asian quake hit european share shares europes ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>business</td>\n",
              "      <td>276.txt</td>\n",
              "      <td>India power shares jump on debut  Shares in In...</td>\n",
              "      <td>india power share jump debut shares indias lar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>business</td>\n",
              "      <td>510.txt</td>\n",
              "      <td>Lacroix label bought by US firm  Luxury goods ...</td>\n",
              "      <td>lacroix label bought us firm luxury good group...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>tech</td>\n",
              "      <td>086.txt</td>\n",
              "      <td>Warning over Windows Word files  Writing a Mic...</td>\n",
              "      <td>warning windows word file writing microsoft wo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>tech</td>\n",
              "      <td>253.txt</td>\n",
              "      <td>Fast lifts rise into record books  Two high-sp...</td>\n",
              "      <td>fast lift rise record book two highspeed lift ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2222</th>\n",
              "      <td>tech</td>\n",
              "      <td>247.txt</td>\n",
              "      <td>Nintendo adds media playing to DS  Nintendo is...</td>\n",
              "      <td>nintendo add medium playing ds nintendo releas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2223</th>\n",
              "      <td>tech</td>\n",
              "      <td>290.txt</td>\n",
              "      <td>Fast moving phone viruses appear  Security fir...</td>\n",
              "      <td>fast moving phone virus appear security firm w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2224</th>\n",
              "      <td>tech</td>\n",
              "      <td>284.txt</td>\n",
              "      <td>Hacker threat to Apple's iTunes  Users of Appl...</td>\n",
              "      <td>hacker threat apples itunes users apples music...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2225 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b31022d8-d7e5-4aad-979a-1bc2c4c34f8c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b31022d8-d7e5-4aad-979a-1bc2c4c34f8c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b31022d8-d7e5-4aad-979a-1bc2c4c34f8c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      category file_name                                               text  \\\n",
              "0     business   289.txt  UK economy facing 'major risks'  The UK manufa...   \n",
              "1     business   504.txt  Aids and climate top Davos agenda  Climate cha...   \n",
              "2     business   262.txt  Asian quake hits European shares  Shares in Eu...   \n",
              "3     business   276.txt  India power shares jump on debut  Shares in In...   \n",
              "4     business   510.txt  Lacroix label bought by US firm  Luxury goods ...   \n",
              "...        ...       ...                                                ...   \n",
              "2220      tech   086.txt  Warning over Windows Word files  Writing a Mic...   \n",
              "2221      tech   253.txt  Fast lifts rise into record books  Two high-sp...   \n",
              "2222      tech   247.txt  Nintendo adds media playing to DS  Nintendo is...   \n",
              "2223      tech   290.txt  Fast moving phone viruses appear  Security fir...   \n",
              "2224      tech   284.txt  Hacker threat to Apple's iTunes  Users of Appl...   \n",
              "\n",
              "                                             text_clean  \n",
              "0     uk economy facing major risk the uk manufactur...  \n",
              "1     aids climate top davos agenda climate change f...  \n",
              "2     asian quake hit european share shares europes ...  \n",
              "3     india power share jump debut shares indias lar...  \n",
              "4     lacroix label bought us firm luxury good group...  \n",
              "...                                                 ...  \n",
              "2220  warning windows word file writing microsoft wo...  \n",
              "2221  fast lift rise record book two highspeed lift ...  \n",
              "2222  nintendo add medium playing ds nintendo releas...  \n",
              "2223  fast moving phone virus appear security firm w...  \n",
              "2224  hacker threat apples itunes users apples music...  \n",
              "\n",
              "[2225 rows x 4 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Displaying the dataframe which contains the processed text\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC0skieVIWMz"
      },
      "source": [
        "# Separating training, development and testing dataset\n",
        "\n",
        "`shuffle` function is used to shuffle the data present in dataframe\n",
        "\n",
        "\n",
        "we are splitting data in the ration **80:10:10**\n",
        "\n",
        "**80%** Train set\n",
        "\n",
        "**10%** Development set\n",
        "\n",
        "**10%** Test\n",
        "\n",
        "---\n",
        "\n",
        "X_train_text  = Contains all the processed text from train set \n",
        "\n",
        "Y_train_label = Contains target labels from train test \n",
        "\n",
        " \n",
        "\n",
        "X_dev_text = Contains all the processed text from dev set \n",
        "\n",
        "Y_dev_Label = Contains target labels from dev set \n",
        "\n",
        " \n",
        "\n",
        "X_test_text = Constians all the processed text from test set \n",
        "\n",
        "Y_test_label = Contains target labels from test set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O-Vz1X2JLhW",
        "outputId": "d808c9a2-0739-4372-c35a-710ba8de5d45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of Train Set is  (1780, 4)\n",
            "Shape of Dev Set is  (222, 4)\n",
            "Shape of Test Set is  (223, 4)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# shuffiling the whole dataframe\n",
        "df = shuffle(df,random_state=11)\n",
        "\n",
        "\n",
        "# considering 80% as the train size\n",
        "train_size = int(len(df)*0.8)\n",
        "\n",
        "# considering 10% as the development size\n",
        "dev_size = int(len(df)*0.1)\n",
        "\n",
        "# considering 10% as the test size\n",
        "test_size = int(len(df)*0.1)\n",
        "\n",
        "\n",
        "# iloc is used to slice the rows [0:80%]\n",
        "train_set = df.iloc[:train_size]\n",
        "\n",
        "# Assigning next 10% to dev_set variable [80% : 90%] \n",
        "dev_set = df.iloc[train_size: train_size + dev_size]\n",
        "\n",
        "# Assigning remaining records to test_set variable [90% : 100%]\n",
        "test_set = df.iloc[ train_size + dev_size: ]\n",
        "\n",
        "\n",
        "\n",
        "print(\"Shape of Train Set is \",train_set.shape)\n",
        "\n",
        "print(\"Shape of Dev Set is \",dev_set.shape)\n",
        "\n",
        "print(\"Shape of Test Set is \",test_set.shape)\n",
        "\n",
        "\n",
        "# Separating the training features\n",
        "X_train_text = train_set[\"text_clean\"]\n",
        "# Separating the target labels\n",
        "Y_train_label = train_set[\"category\"]\n",
        "\n",
        "# Separating the dev features\n",
        "X_dev_text = dev_set[\"text_clean\"]\n",
        "# Separating the dev target labels\n",
        "Y_dev_label = dev_set[\"category\"]\n",
        "\n",
        "# Separating the test features\n",
        "X_test_text = test_set[\"text_clean\"]\n",
        "# Separating the test target labels\n",
        "Y_test_label = test_set[\"category\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7Eid8C9_HfL"
      },
      "source": [
        "`combine_feature` function is a utility function which combines the vector horizontally \n",
        "\n",
        "Example:\n",
        "\n",
        "a = `[[0,2],[2,3]]`\n",
        "\n",
        "b = `[[3,4],[5,6]]`\n",
        "\n",
        "combine_feature returns = `[[0,2,3,4],[2,3,5,6]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G6TcXxGs--tJ"
      },
      "outputs": [],
      "source": [
        "# Utility function which combines vectors\n",
        "def combine_features(*vectors):      \n",
        "    return np.hstack(vectors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZhx0rAoACEf"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "we are using SVC as the machine learning model.\n",
        "\n",
        "**Feature Selection Technique:** we are using `CountVectorizer` `TfidVectorizer(Unigram)` `TfidVectorizer(Bigram)` to convert text content to vectors and using `SelectKBest`, `chi2` to reduce the dimensionality.\n",
        "\n",
        "\n",
        "In this technique we are selecting k best best features for `CountVectorizer` `TfidVectorizer(Unigram)` `TfidVectorizer(Bigram)` and combining those features to one feature vector.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "**CountVectorizer** generated some 10000 feature. By using `chi2` and `selectKBest` we are picking 500 best feature vectors.\n",
        "\n",
        "**TFidVectorizer(Unigram)** generated some 9000 feature. By using `chi2` and `selectKBest` we are picking 500 best feature vectors.\n",
        "\n",
        "**TFidVectorizer(Bigram)** generated some 5000 feature. By using `chi2` and `selectKBest` we are picking 500 best feature vectors.\n",
        "\n",
        "we combine all those best features into one single featuer vector of size 1500\n",
        "\n",
        "we will train our algorithm with these 1500 features.\n",
        "\n",
        "**HyperTuning**: \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcPw9uBJ03xA",
        "outputId": "18c77fd1-a4a6-45b9-89e9-66bede9a4b0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with SVC as the model\n",
            "------------------------------------------\n",
            "Count Vectorizer: \n",
            "\t Reducing features from (1780, 7835) to (1780, 100) using chi2\n",
            "Tfid Vectorizer(Uni-gram): \n",
            "\t Reducing features from (1780, 11377) to (1780, 100) using chi2\n",
            "Tfid Vectorizer(Bi-gram): \n",
            "\t Reducing features from (1780, 5466) to (1780, 100) using chi2 \n",
            "\n",
            "Combined features are of shape (1780, 300)\n",
            "\n",
            "Results for SVM on DEV set is with k = 100 \n",
            "-----------------------------------------\n",
            "Precision: 0.963\n",
            "Recall: 0.965\n",
            "F1-Score: 0.964\n",
            "Accuracy: 0.964\n",
            "\n",
            "\n",
            "Training with SVC as the model\n",
            "------------------------------------------\n",
            "Count Vectorizer: \n",
            "\t Reducing features from (1780, 7835) to (1780, 500) using chi2\n",
            "Tfid Vectorizer(Uni-gram): \n",
            "\t Reducing features from (1780, 11377) to (1780, 500) using chi2\n",
            "Tfid Vectorizer(Bi-gram): \n",
            "\t Reducing features from (1780, 5466) to (1780, 500) using chi2 \n",
            "\n",
            "Combined features are of shape (1780, 1500)\n",
            "\n",
            "Results for SVM on DEV set is with k = 500 \n",
            "-----------------------------------------\n",
            "Precision: 0.976\n",
            "Recall: 0.977\n",
            "F1-Score: 0.977\n",
            "Accuracy: 0.977\n",
            "\n",
            "\n",
            "Training with SVC as the model\n",
            "------------------------------------------\n",
            "Count Vectorizer: \n",
            "\t Reducing features from (1780, 7835) to (1780, 1000) using chi2\n",
            "Tfid Vectorizer(Uni-gram): \n",
            "\t Reducing features from (1780, 11377) to (1780, 1000) using chi2\n",
            "Tfid Vectorizer(Bi-gram): \n",
            "\t Reducing features from (1780, 5466) to (1780, 1000) using chi2 \n",
            "\n",
            "Combined features are of shape (1780, 3000)\n",
            "\n",
            "Results for SVM on DEV set is with k = 1000 \n",
            "-----------------------------------------\n",
            "Precision: 0.971\n",
            "Recall: 0.974\n",
            "F1-Score: 0.972\n",
            "Accuracy: 0.973\n",
            "\n",
            "\n",
            "Training with SVC as the model\n",
            "------------------------------------------\n",
            "Count Vectorizer: \n",
            "\t Reducing features from (1780, 7835) to (1780, 2000) using chi2\n",
            "Tfid Vectorizer(Uni-gram): \n",
            "\t Reducing features from (1780, 11377) to (1780, 2000) using chi2\n",
            "Tfid Vectorizer(Bi-gram): \n",
            "\t Reducing features from (1780, 5466) to (1780, 2000) using chi2 \n",
            "\n",
            "Combined features are of shape (1780, 6000)\n",
            "\n",
            "Results for SVM on DEV set is with k = 2000 \n",
            "-----------------------------------------\n",
            "Precision: 0.981\n",
            "Recall: 0.982\n",
            "F1-Score: 0.981\n",
            "Accuracy: 0.982\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# One of the feature selection technique is based on word frequency. \n",
        "count_vectorizer = CountVectorizer(ngram_range= (1,1),min_df =5)\n",
        "\n",
        "# Creating a vector object that does unigram conversion\n",
        "unigram_vectorizer = TfidfVectorizer(sublinear_tf=True, norm='l2', min_df =3, ngram_range= (1,1))\n",
        "\n",
        "# Creating a bigram object that does bigram conversion\n",
        "bigram_vectorizer = TfidfVectorizer(sublinear_tf=True, norm='l2', min_df =5, ngram_range= (2,2))\n",
        "\n",
        "\n",
        "model_svc_classifier = sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
        "\n",
        "\n",
        "# Hyper tuning parametes by tuning the vector values\n",
        "k_best = [100,500, 1000 ,2000]\n",
        "\n",
        "# Iterating through all the objects\n",
        "for k in k_best:\n",
        "\n",
        "      print(f\"Training with SVC as the model\")\n",
        "      print(\"------------------------------------------\")\n",
        "\n",
        "      # Creating objects which picks k best features\n",
        "      count_best = SelectKBest(chi2, k=k)\n",
        "      unigram_best = SelectKBest(chi2, k=k)\n",
        "      bigram_best = SelectKBest(chi2, k=k)\n",
        "\n",
        "\n",
        "      # Converting train text features to vectors features - Count Vectorizer\n",
        "      count_vector_features = count_vectorizer.fit_transform(X_train_text).toarray()\n",
        "      # Below line select the K best features\n",
        "      count_best_features= count_best.fit_transform(count_vector_features, Y_train_label)\n",
        "      print(f\"Count Vectorizer: \\n\\t Reducing features from {count_vector_features.shape} to {count_best_features.shape} using chi2\")\n",
        "\n",
        "      # Converting train text features to vectors features - TFid Vectorizer unigram\n",
        "      unigram_vector_features = unigram_vectorizer.fit_transform(X_train_text).toarray()\n",
        "      # Below line select the K best features\n",
        "      unigram_best_features = unigram_best.fit_transform(unigram_vector_features, Y_train_label)\n",
        "      print(f\"Tfid Vectorizer(Uni-gram): \\n\\t Reducing features from {unigram_vector_features.shape} to {unigram_best_features.shape} using chi2\")\n",
        "      \n",
        "      # Converting train text features to vectors features - TFid Vectorizer bigram\n",
        "      bigram_vector_features = bigram_vectorizer.fit_transform(X_train_text).toarray()\n",
        "      # Below line select the K best features\n",
        "      bigram_best_features = bigram_best.fit_transform(bigram_vector_features, Y_train_label)\n",
        "      print(f\"Tfid Vectorizer(Bi-gram): \\n\\t Reducing features from {bigram_vector_features.shape} to {bigram_best_features.shape} using chi2 \\n\")\n",
        "\n",
        "\n",
        "      # Combining all the best features selected\n",
        "      X_train_combined_vectors = combine_features(count_best_features,unigram_best_features,bigram_best_features)\n",
        "\n",
        "     \n",
        "      print (f\"Combined features are of shape {str(X_train_combined_vectors.shape)}\\n\")\n",
        "\n",
        "\n",
        "      # training our model with fit method using train dataset\n",
        "      model_svc_classifier.fit(X_train_combined_vectors,Y_train_label)\n",
        "\n",
        "      \n",
        "      #----------------------------------Transforming Dev data set--------------------------------------------\n",
        "\n",
        "\n",
        "    \n",
        "      # Transforming dev set to vectors\n",
        "      count_vector_features = count_vectorizer.transform(X_dev_text).toarray()\n",
        "      # Selecting k best features dev set\n",
        "      count_best_features = count_best.transform(count_vector_features)\n",
        "\n",
        "      # Transforming dev set to vectors\n",
        "      unigram_vector_features = unigram_vectorizer.transform(X_dev_text).toarray()\n",
        "      # Selecting k best features dev set\n",
        "      unigram_best_features = unigram_best.transform(unigram_vector_features)\n",
        "\n",
        "    \n",
        "      # Transforming dev set to vectors\n",
        "      bigram_vector_features = bigram_vectorizer.transform(X_dev_text).toarray()\n",
        "      # Selecting k best features dev set\n",
        "      bigram_best_features = bigram_best.transform(bigram_vector_features)\n",
        "\n",
        "\n",
        "\n",
        "      # Combining all the best featues selected\n",
        "      X_dev_combined_vectors = combine_features(count_best_features,unigram_best_features,bigram_best_features)\n",
        "\n",
        "      \n",
        "      #------------------------------------Validation of DEV data set------------------------------------\n",
        "      \n",
        "      # predicting the output of dev data set\n",
        "      Y_pred_dev =  model_svc_classifier.predict(X_dev_combined_vectors)\n",
        "\n",
        "      accuracy_dev = accuracy_score(Y_dev_label,Y_pred_dev)\n",
        "     \n",
        "      print(f\"Results for SVM on DEV set is with k = {k} \")\n",
        "      print(\"-----------------------------------------\")\n",
        "\n",
        "      precision=precision_score(Y_pred_dev, Y_dev_label, average='macro')\n",
        "      recall=recall_score(Y_pred_dev, Y_dev_label, average='macro')\n",
        "      f1=f1_score(Y_pred_dev, Y_dev_label, average='macro')\n",
        "      accuracy=accuracy_score(Y_pred_dev, Y_dev_label)\n",
        "\n",
        "      print (\"Precision: \"+str(round(precision,3)))\n",
        "      print (\"Recall: \"+str(round(recall,3)))\n",
        "      print (\"F1-Score: \"+str(round(f1,3)))\n",
        "      print (\"Accuracy: \"+str(round(accuracy,3))+'\\n\\n')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcMG5Q8fF8Hk"
      },
      "source": [
        "#Testing our model on Test data set\n",
        "\n",
        "\n",
        "we are getting good accuray for k == 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JpYVzMv8van",
        "outputId": "b6763693-ced5-45f6-b6a4-3ae97bd82954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for SVM on Test set is\n",
            "-----------------------------------------\n",
            "Precision: 0.965\n",
            "Recall: 0.966\n",
            "F1-Score: 0.965\n",
            "Accuracy: 0.964\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "#-----------Transforming Test Data set---------------\n",
        "\n",
        "count_vector_features = count_vectorizer.transform(X_test_text).toarray()\n",
        "count_best_features = count_best.transform(count_vector_features)\n",
        "\n",
        "unigram_vector_features = unigram_vectorizer.transform(X_test_text).toarray()\n",
        "unigram_best_features = unigram_best.transform(unigram_vector_features)\n",
        "\n",
        "bigram_vector_features = bigram_vectorizer.transform(X_test_text).toarray()\n",
        "bigram_best_features = bigram_best.transform(bigram_vector_features)\n",
        "\n",
        "\n",
        "\n",
        "# Transforming test text features to vectors\n",
        "X_test_combined_vectors = combine_features(count_best_features,unigram_best_features,bigram_best_features)\n",
        "\n",
        "# predicting the output of test data set\n",
        "Y_pred_test =  model_svc_classifier.predict(X_test_combined_vectors)\n",
        "\n",
        "\n",
        "print(f\"Results for SVM on Test set is\")\n",
        "print(\"-----------------------------------------\")\n",
        "\n",
        "precision=precision_score(Y_pred_test, Y_test_label, average='macro')\n",
        "recall=recall_score(Y_pred_test, Y_test_label, average='macro')\n",
        "f1=f1_score(Y_pred_test, Y_test_label, average='macro')\n",
        "accuracy=accuracy_score(Y_pred_test, Y_test_label)\n",
        "\n",
        "print (\"Precision: \"+str(round(precision,3)))\n",
        "print (\"Recall: \"+str(round(recall,3)))\n",
        "print (\"F1-Score: \"+str(round(f1,3)))\n",
        "print (\"Accuracy: \"+str(round(accuracy,3))+'\\n\\n')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bbc_text.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
